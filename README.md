# Проект: Классификация токсичных комментариев

## Описание проекта

Интернет-магазин **«Викишоп»** запускает сервис коллективного редактирования описаний товаров. Пользователи могут оставлять комментарии, предлагать правки и обсуждать изменения. Чтобы защитить сообщество от токсичных высказываний, необходимо автоматизировать процесс модерации.

Цель проекта — построить модель, определяющую токсичность комментариев и отправляющую потенциально оскорбительные сообщения на модерацию.

## Постановка задачи

- Обучить модель бинарной классификации (`токсичный` / `нетоксичный`) на размеченном датасете.
- Добиться **значения метрики F1 ≥ 0.75** на валидационной выборке.
- Выявить слова-индикаторы токсичности.
- Сравнить несколько моделей и выбрать наилучшую по качеству и интерпретируемости.

## Описание данных

Исходный файл: **`toxic_comments.csv`**

- `text` — текст пользовательского комментария.
- `toxic` — бинарная метка (1 — токсичный, 0 — нет).

Наблюдается сильный **дисбаланс классов** (нетоксичных существенно больше), что требует учёта при обучении моделей.

## Как решалась задача

### 1. Подготовка данных

- Импортированы библиотеки: `Pandas`, `Numpy`, `NLTK`, `Sklearn`, `CatBoost`.
- Проведена **лемматизация** и удаление стоп-слов.
- Тексты приведены к нижнему регистру, очищены от спецсимволов.
- Использован `TfidfVectorizer` для преобразования текста в векторы признаков.

### 2. Балансировка классов

- Для борьбы с дисбалансом использован подход с `class_weight=balanced` и кастомными метриками.
- Также экспериментально применялись методы undersampling/oversampling (опционально).

### 3. Обучение моделей

Сравнивались 3 модели:

- **Logistic Regression** — простая и интерпретируемая модель.
- **LinearSVC** — устойчивый классификатор для текстов.
- **CatBoostClassifier** — градиентный бустинг, способный учитывать категориальные особенности и нелинейные зависимости.


### 4. Интерпретация модели

- Построены графики важности признаков (слов).
- Наиболее токсичные слова: **“fuck”**, **“stupid”**, **“idiot”**, и др.
- Модели по-разному интерпретируют вес слов:
  - Logistic Regression — линейно и явно.
  - LinearSVC — мягче.
  - CatBoost — нелинейная зависимость и сглаживание.

## Использованные технологии

- `Python`, `Pandas`, `NLTK`,  `CatBoost`
- `scikit-learn` (`TF–IDF`, `Pipeline`, `StratifiedKFold`, `GridSearchCV`, `LogisticRegression`, `SGDClassifier`, `LinearSVC`, `RandomForestClassifier`, `CalibratedClassifierCV`)  
- `TfidfVectorizer`, `GridSearchCV`
- - `CatBoost` (`CatBoostClassifier`, `Pool`, `cv`)  
- `scipy.sparse`  
- `PyTorch` (`torch`)  

## Результаты

- **Модель CatBoost показала наилучший результат:**  
  `F1 ≈ 0.78` 
  `ROC-AUC = 0.96`
- Модель устойчива к дисбалансу классов.
- Проведён анализ значимых признаков — выявлены ключевые слова токсичности.
- Подготовлен готовый пайплайн для обработки новых комментариев.

## Выводы

- Автоматическая классификация токсичных комментариев возможна с высокой точностью.
- Модель CatBoost показала отличную адаптацию к текстам и дисбалансу.
- Инструмент может быть внедрён в backend платформы для фильтрации и предупреждения пользователей.
- В перспективе можно добавить обучение на большем корпусе и попробовать BERT/Transformer-модели.
